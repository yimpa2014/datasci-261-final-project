{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93cbedd3-613f-49f5-811e-27136f691e4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Predict Flight Delay: Team 1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "945340db-d66d-4611-9e35-00de87ad0511",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Team Information\n",
    "#### Meet the Team:\n",
    "| Name | Email | Photo | Location|\n",
    "|----------------|----------------------------|--------------------|--------------------|\n",
    "| Alec Naidoo    | alecarnassi@berkeley.edu   |          <img src=\"https://raw.githubusercontent.com/Alec12/261-final-project-1-2/main/profilepic1.png?raw=true)\" width=\"100\" height=\"100\">          | San Francisco, CA  |\n",
    "| Jian Wang      | j955wang@berkeley.edu      |         <img src=\"https://ca.slack-edge.com/T0WA5NWKG-U04355YF7RT-dfd74d0d5ae8-512\" width=\"100\" height=\"100\">           | Vancouver, BC      |\n",
    "| Patrick Yim    | yimpa2014@berkeley.edu     | <img src=\"https://ca.slack-edge.com/T0WA5NWKG-U047UTGDQ03-39a3abbe2c87-512\" alt=\"Patrick Yim\" width=\"100\" height=\"100\"> | San Francisco, CA  | \n",
    "| Trisha Sanghal | trishasanghal@berkeley.edu |          <img src=\"https://ca.slack-edge.com/T0WA5NWKG-U05GN15UGBE-9e2225fb6ef6-512\" width=\"100\" height=\"100\">          |          Austin, TX          | \n",
    "\n",
    "#### Phase 3 Leaders: \n",
    "Jian Wang, Patrick Yim\n",
    "\n",
    "#### Detailed Phase Leader Plan:\n",
    "| Week | Phase | Leader        | Backup/Co-Leader  |\n",
    "|------|-------|---------------|-------------------|\n",
    "| 1    | 1     | Patrick Yim   | Trisha Sanghal    |\n",
    "| 2    | 2     | Trisha Sanghal| Alec Naidoo       |\n",
    "| 3    | 2     | Alec Naidoo   | Jian Wang         |\n",
    "| 4    | 3     | Jian Wang     | Patrick Yim       |\n",
    "| 5    | 3     | Patrick Yim   | Trisha Sanghal    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39e03a70-3c80-4b35-a708-f503493d2757",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Credit Assignment Plan\n",
    "\n",
    "#### Credit Assignment Plan Phase 2:\n",
    "|Task|Contributers|Start Date|End Date|Contribution Hours|\n",
    "|--------|-----------|----------|----------|----------|\n",
    "|Making Adjustments to Project Plan After Phase 1 Feedback|Alec, Jian, Patrick, Trisha|7/15|7/22|2|\n",
    "|More In-Depth EDA|Alec, Patrick, Trisha|7/15|7/22|10|\n",
    "|Feature Engineering|Alec, Trisha|7/15|7/25|5|\n",
    "|Configuring Baseline Model|Alec, Jian|7/15|7/22|3|\n",
    "|Proposed Model Testing|Alec, Jian, Patrick|7/15|7/28|10|\n",
    "|Working on Deliverable for Phase 2|Alec, Jian, Patrick, Trisha|7/23|7/28|15|\n",
    "\n",
    "#### Credit Assignment Plan Phase 3:\n",
    "|Task|Contributers|Start Date|End Date|Contribution Hours|\n",
    "|----------|------------|-----------|-----------|-----------|\n",
    "| Making Adjustments to Project Plan After Phase 2 Feedback | Alec, Jian, Patrick, Trisha |7/29|8/4|2|\n",
    "| Feature Refinement | Patrick |7/29|8/4|3|\n",
    "| Configuring Neural Net Models | Alec |7/29|8/4|3|\n",
    "| Configuring Additional Complex Models | Trisha |8/4|8/10|3|\n",
    "| Fine-tuning Modeling Pipelines | Jian |8/4|8/10|3|\n",
    "| Working on Deliverable for Phase 3 | Alec, Jian, Patrick, Trisha |8/4|8/10| 10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29e68785-c9e9-4f4b-aa21-1994aed6c59d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Abstract\n",
    "Accurate flight delay predictions can be vital to the success of airlines for multiple reasons: (1) airlines can optimize their resource and personnel allocation to minimize disruption to passengers, (2) airlines can make more informed long-term decisions about infrastructure improvements, and (3) findings from previous flight delay responses can help airlines advance research and development across the broader aviation industry. To help contribute to the success of airlines, our project's aim is to produce a binary classification model that predicts whether or not a flight will experience arrival delay, defined as a 15-minute or greater difference between the planned and actual arrival times. We believe that the binary classification model will produce predictions that are easy to interpret and action on (airlines only need to know whether the delay exceeds the 15-minute threshold to decide whether to implement targeted responses), and will be better suited than a regression model to handle potential data imbalances in arrival delay. To accomplish this goal, we will leverage the following three datasets: flight on-time performance data from the U.S. Department of Transportation, weather data corresponding to origin and destination airports from the National Oceanic and Atmospheric Administration Repository, and metadata about airports in the U.S. from the U.S. Department of Transportation. These datasets contain data from 2015 to 2019 for flights within the United States.\n",
    "\n",
    "Our first baseline model predicts delay or no delay at random, and has an F1 score of 55.14% on the test set (year 2019 from the 2015-2019 OTPW data). Our second baseline model always predicts the majority class of no delay, and has an F1 score of 71.96% on the test set. We selected these models as our baselines because they are simple to implement and understand, and they provide us with a reasonable minimal performance level (if our model doesn't outperform the baselines, it suggests that our model isn't capturing useful patterns from the data). We also implemented Logistic, Random Forest, Neural Network classification models with F1 scores of 78.15%, 72.54%, and 81.23% respectively on the test set. Our final model with the highest F1 score was therefore the Neural Network Classifier, and some of the most important features used as inputs to this model were the airline performance, holiday indicator, and origin airport page rank. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a7edc94-f0c5-482c-868e-27b39e0488c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data and Feature Engineering\n",
    "#### Data Source\n",
    "In this project, we will leverage the following datasets: \n",
    "| Dataset | Source | Timeframe Available | Description | Dimensions\n",
    "|------|-------|---------------|-------------------|----------|\n",
    "|Flights|TranStats Data Collection from the U.S. Department of Transportation|2015 to 2019|Contains flight on-time performance data within the U.S.|31,746,841 x 109|\n",
    "|Weather|National Oceanic and Atmospheric Administration Repository|2015 to 2019|Contains weather data corresponding to origin and destination airports at departure and arrival times.|630,904,436 x 124|\n",
    "|Stations|U.S. Department of Transportation|Last Updated 2024|Contains metadata about airports in the U.S.|18,097 x 12|\n",
    "|OTPW (Ontime Performance of Flights and Weather)|261 Instructors|2015 to 2019|Contains joined data from the flights, weather, and stations tables.|31673119 x 214|\n",
    "\n",
    "These datasets have been provided to us across shorter timeframes as well (including the first 3 months in 2015, the first 6 months in 2015, and 1 year in 2015). \n",
    "\n",
    "#### Data Split\n",
    "Before training our models, we split the 2015-2019 OTPW data into train, validation, and test sets:\n",
    "- The **train** set is composed of 2015-2018 OTPW data.\n",
    "- To **validate** our model on the train data, we use a blocked time series split approach to cross-validation. More specific implementation details are provided in the Modeling Pipelines section below.\n",
    "- The **test** set is composed of 2019 OTPW data.\n",
    "\n",
    "#### Target Variable\n",
    "Our task is to produce a binary classification model that can predict whether or not a flight will experience arrival delay, where arrival delay is defined as a 15-minute or greater difference between the planned and actual arrival times. We chose to approach this project as a classification problem for a few reasons:\n",
    "1. Binary classification models will produce predictions that are easy to interpret and action on (airlines only need to know whether the delay exceeds the 15-minute threshold to decide whether to implement targeted responses).\n",
    "2. Binary classification models are better suited than regression models to handling data imbalances in arrival delay.\n",
    "3. Managing passenger expectations is easier with binary results. \n",
    "\n",
    "Below are pie charts demonstrating the proportions and counts of our target variable, 'ARR_DEL_15', across multiple years of the train set. As expected, there is not much variation in the proportion of delays from year to year even though the total number of flights is steadily increasing. This consistent distribution across the years suggests stability in the underlying factors influencing flight delays. It indicates that the model’s predictions will be consistent over time, which can provide reliable insights for the airlines regardless of the specific year. It also implies that sudden changes on a day to day basis do not dramatically affecting delay occurrences, which is essential for our predictive modeling.\n",
    "\n",
    "&nbsp;\n",
    "<img src=\"https://raw.githubusercontent.com/TSanghal/261-final-project-1-2/main/2015.png\" width=\"300\" height=\"300\">\n",
    "<img src=\"https://raw.githubusercontent.com/TSanghal/261-final-project-1-2/main/2016.png\" width=\"300\" height=\"300\">\n",
    "<img src=\"https://raw.githubusercontent.com/TSanghal/261-final-project-1-2/main/2017.png\" width=\"300\" height=\"300\">\n",
    "<img src=\"https://raw.githubusercontent.com/TSanghal/261-final-project-1-2/main/2018.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "#### Missing Values Analysis\n",
    "Because of the significant amount of missing data in the dataset, we began our EDA with a missing value analysis. The following plot depicts the percentage of values present in each column of the train set (2015-2018 OTPW data). Using these percentages, we filtered out columns that were missing more than 50% of their values. \n",
    "<img src=\"https://raw.githubusercontent.com/TSanghal/261-final-project-1-2/main/Missing_Values_Visualization.png\" width=\"700\" height=\"700\">\n",
    "\n",
    "The reasons for quantity of missing data can vary, but a closer examination of the columns with 100% missing values reveals that many of these are related to weather conditions. Some ideas on why the data is missing could be that the technology that is capturing Snowfall and SeaLevelPressure does not exist, or is malfunctioning across geographic locations. Human error is probably not as relevant for some of these columns, as the data is likely collected in an automated job queueing system. \n",
    "\n",
    "&nbsp;\n",
    "<img src=\"https://raw.githubusercontent.com/yimpa2014/datasci261_py_image/main/Screenshot%202024-07-28%20170037.png\">\n",
    "&nbsp;\n",
    "\n",
    "The following data dictionary summarizes the columns remaining after removing columns missing more than 50% of their values. \n",
    "| Dataset | Total # of Columns | # of Columns with < 50% Null Values | Names of Columns with < 50% Null Values |\n",
    "|------|-------|-------|-------|\n",
    "|Flights|109|56|'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'OP_UNIQUE_CARRIER', 'OP_CARRIER_AIRLINE_ID', 'OP_CARRIER', 'TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR', 'ORIGIN_STATE_FIPS', 'ORIGIN_STATE_NM', 'ORIGIN_WAC', 'DEST_AIRPORT_ID', 'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID', 'DEST', 'DEST_CITY_NAME', 'DEST_STATE_ABR', 'DEST_STATE_FIPS', 'DEST_STATE_NM', 'DEST_WAC', 'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW', 'DEP_DEL15', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', 'TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON', 'TAXI_IN', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'ARR_DELAY_NEW', 'ARR_DEL15', 'ARR_DELAY_GROUP', 'ARR_TIME_BLK', 'CANCELLED', 'DIVERTED', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'FLIGHTS', 'DISTANCE', 'DISTANCE_GROUP', 'DIV_AIRPORT_LANDINGS', 'YEAR'|\n",
    "|Weather|124|19|'STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'NAME', 'REPORT_TYPE', 'SOURCE', 'HourlyAltimeterSetting', 'HourlyDewPointTemperature', 'HourlyDryBulbTemperature', 'HourlyRelativeHumidity', 'HourlyStationPressure', 'HourlyVisibility', 'HourlyWetBulbTemperature', 'HourlyWindDirection', 'HourlyWindSpeed', 'REM', 'YEAR'|\n",
    "|Stations|12|12|'usaf', 'wban', 'station_id', 'lat, 'lon', 'neighbor_id', 'neighbor_name', 'neighbor_state', 'neighbor_call', 'neighbor_lat', 'neighbor_lon', 'distance_to_neighbor'|\n",
    "\n",
    "#### Numerical Features\n",
    "Numerical features, also known as quantitative features, are variables in the OTPW dataset that represent measurable quantities and are expressed as numbers. \n",
    "\n",
    "Below is a table summarizing how we addressed missing values for the numerical features of the 2015-2019 OTPW data. We calculated all statistics (e.g. mean) using the train set (2015-2018 OTPW data) and applied them to both the train and test (2019 OTPW data) sets.\n",
    "\n",
    "| Feature Name (Some Missing %)                                                                 | Action             | Reasoning                                                                                                      |\n",
    "|------------------------------------------------------------------------------|--------------------|---------------------------------------------------------------------------------------------------------------|\n",
    "| CARRIER_DELAY, LATE_AIRCRAFT_DELAY, WEATHER_DELAY, SECURITY_DELAY, NAS_DELAY | Drop               | High missing values; Data leakage.                                                                   |\n",
    "| ARR_DELAY, ACTUAL_ELAPSED_TIME, AIR_TIME, ARR_DELAY_NEW, ARR_TIME, TAXI_IN, WHEELS_ON, WHEELS_OFF, TAXI_OUT, DEP_DELAY, DEP_DELAY_NEW, DEP_TIME | Drop | Data Leakage. |\n",
    "| BackupElevation, BackupLatitude, BackupLongitude | Drop               | High missing values; less relevant. \n",
    "| HourlyPressureChange                                 | Impute mean        | Weather-related; might have predictive power.                                                                  |\n",
    "| HourlyPrecipitation, HourlySeaLevelPressure, HourlyAltimeterSetting          | Impute mean        | Weather-related; might have predictive power.                                                                  |\n",
    "| HourlyWetBulbTemperature, HourlyStationPressure, HourlyWindDirection, HourlyRelativeHumidity, HourlyWindSpeed, HourlyDewPointTemperature, HourlyDryBulbTemperature, HourlyVisibility | Impute mean | Weather-related; might have predictive power. | \n",
    "| CRS_ELAPSED_TIME                                                             | Impute previous values; impute mean        | Related to scheduled elapsed time; important for analysis. |\n",
    "\n",
    "After handling the missing values, we standardized our numerical features by subtracting the mean and dividing by the standard deviation. Here again, we calculated all statistics (mean and standard deviation) using the train set (2015-2018 OTPW data) and applied them to both the train and test (2019 OTPW data) sets. Standardization helps ensure a consistent scale so that numerical features with different scales don't have uneven influence on the model. \n",
    "\n",
    "&nbsp;\n",
    "<img src=\"https://raw.githubusercontent.com/Alec12/261-final-project-1-2/main/point_biserial.png?raw=true\" width=\"700\" height=\"700\">\n",
    "\n",
    "This bar plot displays the Point-Biserial Correlation Coefficients between our numerical features and the target variable, ARR_DEL15. The Point-Biserial Correlation Coefficient is used here because it measures the relationship between a binary variable (ARR_DEL15) and continuous numerical features, allowing us to understand how certain numerical features relate to flight delays.\n",
    "\n",
    "For example, positive correlations, as seen with CRS_DEP_TIME and CRS_ARR_TIME, suggest that later scheduled departure or arrival times might be associated with delays, which aligns with expectations. On the other hand, features like HourlyVisibility and HourlyDryBulbTemperature show negative correlations, indicating that better visibility or higher temperatures might correspond with fewer delays. These insights are valuable as we move forward with our model development efforts.\n",
    "\n",
    "#### Categorical Features \n",
    "Categorical features, also known as qualitative features, are variables in the OTPW dataset that represent discrete categories or groups. \n",
    "\n",
    "Below is a table summarizing how we addressed missing values for the categorical features of the 2015-2019 OTPW data. \n",
    "| Feature Name (Some Missing %)                                                                 | Action             | Reasoning                                                                                                      |\n",
    "|------------------------------------------------------------------------------|--------------------|---------------------------------------------------------------------------------------------------------------|                                                                   \n",
    "| ARR_DELAY_GROUP, DEP_DELAY_GROUP, DEP_DEL15 | Drop | Data Leakage. |\n",
    "| BackupEquipment, BackupDistanceUnit, BackupDirection, BackupElements, BackupName                         | Drop               | High missing values; less relevant. \n",
    "| BackupDistance                        | Impute 'Unknown'    | High missing values; some distribution.                                                                       |\n",
    "| HourlyPressureTendency | Impute Mode | Weather-related; feature-engineering potential.    |\n",
    "| HourlySkyConditions, WindEquipmentChangeDate               | Impute 'Unknown'    | Weather-related; feature-engineering potential.    | \n",
    "TAIL_NUM                                                                     | Impute 'Unknown'   | Identifier; impute with placeholder.                                                                           |\n",
    "| REM                                                                          | Impute 'Unknown'   | Rare feature; impute with placeholder.                                                                         |   \n",
    "\n",
    "After handling the missing values, we one-hot encoded our categorical features. One-hot encoding converts categorical features into a format that our Logistic and Neural Network classifiers can use (since these machine learning algorithms require numerical input).\n",
    "\n",
    "#### Derived Features \n",
    "Derived features, also known as engineered features, are variables that are created from existing raw columns through various transformations. \n",
    "\n",
    "**Airport PageRank**\n",
    "\n",
    "Our client desired a graph based feature, which we looked to add a nea column and feature around PageRank. This included creating new columns for the origin/destination pagerank, which in this case measures centrality. Airports with higher scores means they have more routed connections, and subsequently, traffic as well. \n",
    "For this same reason, airports with higher Pagerank will likely experience greater delays because of the increased traffic flow. To create PageRank feature, we created vertices for unique airports from origin and destination columns, constructed the graph using vertices and edges, calculated PageRank for origin/destination, and then joined PageRank to the original dataset.\n",
    "Below we see the distribution for PageRank which is quite interesting. A filter for the top origin/destination PageRank revealed that the Atlanta Hartsfield airport is the busiest airport, which was cross-verified through a Google search, which states that it is the busiest airport in the world. One can infer that Atlanta has the most connections and serves as a hub to other major airports within various cities. \n",
    "<img src=\"https://github.com/yimpa2014/datasci261_py_image/blob/main/download%20(1).png?raw=true\" width=\"700\" height=\"700\">\n",
    "\n",
    "\n",
    "**Wind Resistance**\n",
    "\n",
    "\n",
    "We added several other new features based on the client requirements. First and foremost, similar to group we added a variable wind feature that aims to improve our model. We created a direction function that measures the longitude and latitude of the origin and destination and then our wind direction feature. This new column measures wind direction as 1 or 0 based on if it exists or not through calculating if there is a difference between the direction function and wind direction feature. This ultimately helps to infer what headwinds and tailwinds affect flight performance. This again helps improve the overall accuracy of our model. \n",
    "We ran descriptive statistics and also several EDA graphs. Below is one of them, which shows the count of the total 0 and 1's based on the wind direction. We ran correlation matrices and some other tests, and although the direct correlation appears weak with ARR_DELAY15, we are confident that the other interactions with variables with highly influence the overall performance of the model.\n",
    "<img src=\"https://github.com/yimpa2014/datasci261_py_image/blob/main/download%20(4).png?raw=true\" width=\"600\" height=\"600\">\n",
    "\n",
    "**Holiday Indicator**\n",
    "\n",
    "We looked into average flight delay in minutes by day of year using the 2015-2019 OTPW data (summary plot below). We found that a few of the spikes in average arrival delay corresponded to U.S. public holidays. For example, there are spikes on or near days 1 (New Year's Day), 15 (MLK Jr. Day), 50 (President's Day), and 148 (Memorial Day). Because of this, we decided to encode a holiday indicator feature that determined if the flight date falls on or near a holiday.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/TSanghal/261-final-project-1-2/main/Day_of_Year_Full.png\" width=\"700\" height=\"700\">\n",
    "\n",
    "Below is a table summarizing our derived features.\n",
    "|Feature Name|Description|Implementation|Rationale|\n",
    "|---|---|---|---|\n",
    "|Interactive Term (Airport x Departure Time)|Create an interaction term between the airport and the scheduled departure time.|`interaction_airport_departure_time = airport * scheduled_departure_time`|Different airports might have varying levels of congestion at different times of day.|\n",
    "|Wind Resistance|Create Wind Direction Feature to see if there was variable wind.|`Variable_wind = 1 if wind direction is marked variable else 0`|Flights can be impacted by wind causing delays.|\n",
    "|Holiday Indicator|Determine if the flight date falls on or near a holiday.|`is_holiday = 1 if flight_date in holiday_dates else 0`|Flights around holidays may have different delay patterns due to higher travel volumes.|\n",
    "|Weekend Indicator|Determine if the flight date falls on a weekend.|`is_holiday = 1 if flight_date in weekend_dates else 0`|Flights around weekends may have different delay patterns due to higher travel volumes.|\n",
    "|Days Since Equipment Change|Calculate the number of days since the last equipment change.|`Days_since_equipment_change = flight_date - wind_equipment_change_date`|Older equipment might be more prone to causing delays.|\n",
    "|Aircraft Flight Count|Calculate the flight count of the aircraft.|Count the number of flights for each aircraft and update the count for each row.|Older aircrafts might be more prone to mechanical issues causing delays.|\n",
    "|Airline Performance|Capture the historical performance of airlines (7 days).|Average delay time window, on-time performance metrics.|Airlines with better performance records are likely to have less delays.|\n",
    "|Origin Airport PageRank|Capture the importance of the origin airport in the network of airports.|Run PageRank algorithm on network of airports and add a column for the rank of the origin airport of each flight.|Central or critical airports may be more prone to experiencing flight delays.|\n",
    "|Destination Airport PageRank|Capture the importance of the destination airport in the network of airports.|Run PageRank algorithm on network of airports and add a column for the rank of the destination airport of each flight.|Central or critical airports may be more prone to experiencing flight delays.|\n",
    "\n",
    "#### Final Features\n",
    "After all of the above steps, here are the final features we used as inputs to our models (in no particular order): \n",
    "\n",
    "|Feature Name|Feature Family|\n",
    "|---|---|\n",
    "|'CRS_DEP_TIME'|Numerical|\n",
    "|'CRS_ARR_TIME'|Numerical|\n",
    "|'CRS_ELAPSED_TIME'|Numerical|\n",
    "|'ELEVATION'|Numerical|\n",
    "|'DISTANCE'|Numerical|\n",
    "|'HourlySeaLevelPressure'|Numerical|\n",
    "|'HourlyRelativeHumidity'|Numerical|\n",
    "|'HourlyPressureChange'|Numerical|\n",
    "|'HourlyPrecipitation'|Numerical|\n",
    "|'HourlyDryBulbTemperature'|Numerical|\n",
    "|'HourlyDewPointTemperature'|Numerical|\n",
    "|'HourlyAltimeterSetting'|Numerical|\n",
    "|'HourlyVisibility'|Numerical|\n",
    "|'HourlyWindSpeed'|Numerical|\n",
    "|'HourlyWetBulbTemperature'|Numerical|\n",
    "|'HourlyStationPressure'|Numerical|\n",
    "|'HourlyPressureTendency'|Categorical|\n",
    "|'is_weekend'|Derived, Categorical| \n",
    "|'is_holiday'|Derived, Categorical|\n",
    "|'variable_wind'|Derived, Categorical|\n",
    "|'dest_airport_pagerank'|Derived, Categorical|\n",
    "|'origin_airport_pagerank'|Derived, Categorical|\n",
    "|'airline_performance'|Derived, Numerical|\n",
    "|'aircraft_flight_count'|Derived, Numerical|\n",
    "|'days_since_equipment_change'|Derived, Numerical|\n",
    "|'relative_wind_angle'|Derived, Numerical|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44f57c20-fecd-4a6e-9791-522326e94716",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Leakage\n",
    "#### Definition and Examples\n",
    "Data leakage occurs when information outside of the train set is unintentionally used to train a model, and it usually results in inflated performance metrics. There are a few types to be wary of when predicting flight delay:\n",
    "  1. **Data splitting leakage** occurs when there is overlap between the train and test sets. This results in test data being used to train the model.\n",
    "  2. **Preprocessing leakage** occurs when information or statistics from the train _and_ test sets are used to preprocess the train set. This allows the model to capture information about the test set.  \n",
    "  3. **Feature leakage** occurs when input features of the model unknowingly contain information about the target variable that would be unrealistic to have (e.g. accidentally including the target variable itself as an input feature to the model).  \n",
    "  4. **Time-based leakage** occurs when data from the future is used to train the model, resulting in unrealistic predictions (e.g. if a model trained on 2019 flight data was used to predict arrival delay for 2018 flights).\n",
    "\n",
    "#### Leakage in the Pipeline\n",
    "To combat data splitting leakage and time-based leakage, we assigned flights from 2015-2018 to our train set and flights from 2019 to our test set. This split ensured that there was no overlap between the train and test sets, and that all of the train data was from an earlier time period than the test data. To avoid preprocessing leakage, we used information and statistics from only the train set to preprocess our train and test sets. And, to prevent feature leakage, we manually reviewed the final input features and removed any related to flight delay. Through these actions, we validated that our pipeline does not suffer from data leakage. \n",
    "\n",
    "#### Cardinal Sins of Machine Learning\n",
    "We also believe that our pipeline does not violate the cardinal sins of machine learning. Throughout the process of building our model, we did our best to ensure that we were not artificially inflating our performance metrics. For example, we did not intentionally manipulate the random seed when building our random class baseline model in order to achieve a higher F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6684dba-7e1a-4edc-8c57-ee0dd7d28556",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Modeling Pipelines\n",
    "\n",
    "#### Pipeline Diagram\n",
    "The development of our flight delay predictive models involved several critical steps, which are outlined as follows:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/TSanghal/261-final-project-1-2/main/ML_Pipeline_3.png\">\n",
    "\n",
    "<!--- Commented because it seems like this is covered in previous sections, but feel free to uncomment! \n",
    "#### Data ETL Process\n",
    "The initial phase involved extracting data from the OTPW datasets spanning the years 2015-2019. This process required meticulous handling to ensure data integrity. We conducted a thorough missing value analysis to address any gaps in the data, followed by data formatting to ensure consistency across all datasets. This step was crucial for preparing the data for subsequent analysis and modeling.\n",
    "\n",
    "#### Feature Engineering\n",
    "In the feature engineering phase, we enhanced the dataset by creating new variables to potentially improve model performance. This included the following:\n",
    "\n",
    "1. **Categorical Feature Creation**: Certain continuous variables were transformed into categorical ones to capture non-linear relationships. The categorical variables selected for the final model included factors such as \"TAXI_IN,\" \"ARR_TIME,\" \"FLIGHTS,\" and \"DISTANCE GROUP.\"\n",
    "\n",
    "2. **Feature Transformation**: We developed features based on domain knowledge to capture important patterns. For example, holiday-based features were created to account for spikes in flight delays during holidays. We also generated PageRank-based features to assess the contribution of specific flight hubs in predicting delays.\n",
    "\n",
    "3. **Indicator Boolean Variables**: Binary indicators were introduced, such as the number of days since the last equipment change, providing insights into operational aspects that may influence delays.\n",
    "\n",
    "4. **Historical Moving Averages**: We calculated moving averages of historical flight delays for each airplane to capture trends over time, using them as predictors. The rationale is that airlines with a history of delays are more likely to experience delays in the future.\n",
    "\n",
    "#### Model Training\n",
    "A standardized model training pipeline was established to ensure consistency and repeatability across different models. This involved splitting the data into training and validation sets, applying feature scaling, and training all base models under the same conditions. The models used included linear regression, random forests, and neural networks. -->\n",
    "\n",
    "#### Cross Validation with Blocked Time Series Splits\n",
    "\n",
    "![blockedtimeseriescv](https://raw.githubusercontent.com/Alec12/261-final-project-1-2/main/blockedtimeseriescv2.png?raw=true)\n",
    "\n",
    "In our cross-validation strategy, we consider a Blocked Time Series Split approach, as shown in the figure above. We consider 3 splits across each year of our 2015-2018 OTPW data, reserving the 2019 OTPW data for our test dataset. In these 3 splits, we create training and validation datasets, with 80% of the data within the split going towards the training data. This approach ensures that the model is trained and validated on temporally ordered data, which is representative of real-world scenarios where past data is used to predict future outcomes. In doing so, we can assess the model’s performance in a way that mimics actual operational conditions, providing more robust performance metrics.\n",
    "\n",
    "#### Model Implementation Details\n",
    "Using the above steps, we trained the following models (on the 2015-2018 OTPW data): \n",
    "\n",
    "|Algorithm|Input Features|Loss Function|Evaluation Metrics|Build Details|\n",
    "|---|---|---|---|---|\n",
    "|Majority Class Baseline|None|None|- Precision<br> - Recall<br> - Accuracy<br> - **F1 Score**|- Cluster Size: 1 Driver, 2-5 Worker Nodes </br> - Time to Build Model: 0 minutes (no training required)|\n",
    "|Logistic Classifier |- Numerical: 20 </br>- Categorical: 6 </br>- Derived: 9|Log Loss|- Precision<br> - Recall<br> - Accuracy<br> - **F1 Score**|- Cluster Size: 1 Driver, 2-5 Worker Nodes </br> - Time to Build Model: 1 hour |\n",
    "|Random Forest Classifier|- Numerical: 20 </br>- Categorical: 6 </br>- Derived: 9|Entropy|- Precision <br> - Recall <br> - Accuracy<br> - **F1 Score**|- Cluster Size: 1 Driver, 2-5 Worker Nodes </br> - Time to Build Model: 2 hour |\n",
    "|Neural Network Classifier|- Numerical: 20 </br>- Categorical: 6 </br>- Derived: 9|Binary Cross-Entropy Loss|- Precision <br> - Recall <br> - Accuracy<br> - **F1 Score**|- Cluster Size: 1 Driver, 2-5 Worker Nodes </br> - Time to Build Model: 13 hour |\n",
    "\n",
    "Here, the Majority Class Baseline always predicts the class that has the highest count in the train set, which is no delay. The Random Class Baseline predicts the delay and no delay classes with equal probability.\n",
    "\n",
    "The formulas for our evaluation metrics are:\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "$$F_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}  $$\n",
    "where\n",
    "* TP is the number of true positives (predict delay, actual delay),\n",
    "* TN is the  number of true negatives (predict no delay, actual no delay),\n",
    "* FP is the number of false positives (predict delay, actual no delay), and \n",
    "* FN is the number of false negatives (predict no delay, actual delay). \n",
    "\n",
    "#### Target Metric\n",
    "Airlines must balance two perspectives when predicting arrival delays. From the airline’s perspective, false negatives are more costly because they may result in being underprepared for actual delays, whereas false positives only lead to over-preparation when there is no delay. From the consumer perspective, if the model predicts a delay and they prepare for it, but the flight is actually on time, they might miss their flight.\n",
    "\n",
    "Therefore, choosing the **F1-score**, which balances both classes, is extremely important. Minimizing false negatives helps airlines prepare for actual delays. At the same time, we value customer satisfaction, and minimizing false positives is crucial for passenger scheduling. Though this metric will be our primary concern, we will still log other classification metrics like Accuracy, Precision, and Recall in evaluating our models.\n",
    "\n",
    "#### Model Parameter Fine-Tuning\n",
    "\n",
    "For optimizing the performance of our models, we leveraged grid search algorithms, particularly important for complex models such as Neural Networks and Random Forests. Grid search involves systematically exploring a defined parameter space to identify the optimal combination of hyperparameters that maximize the model's predictive performance.\n",
    "\n",
    "To fine-tune our Multilayer Perceptron (MLP) model, we conducted an extensive grid search using the following hyperparameters:\n",
    "\n",
    "Max Iterations (maxIter): This parameter defines the maximum number of iterations the algorithm is allowed to run. We experimented with values of 5, 10, and 20.\n",
    "\n",
    "Step Size (stepSize): This controls the size of the steps taken during optimization, which can affect the convergence rate of the model. We explored step sizes of 0.05, 0.1, and 0.2.\n",
    "\n",
    "Block Size (blockSize): This parameter determines the size of the data chunks processed in parallel by the algorithm, with values of 64, 128, and 256 tested.\n",
    "\n",
    "Additionally, we conducted two experiments with different MLP hidden layer configurations. First, we tested a simpler MLP model with only two layers, consisting of 9 and 3 nodes, respectively. We then hypothesized that increasing the number of layers would allow the model to capture more complexity in the data. In the second model, we introduced a more complex architecture with the following node sequence: [27, 18, 18, 18, 9, 9, 9, 5, 2]. However, after evaluating the F1-Scores on the validation data, we found little to no improvement in performance, indicating that the more complex model did not significantly outperform the simpler one. \n",
    "\n",
    "We integrated these parameters into a time-series-based cross-validation process, which ensured robust model evaluation while avoiding data leakage. By using this approach, we could identify the hyperparameter combination that provided the best model performance, ensuring that our final model was both accurate and generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aed97afc-1f84-4b07-b702-3f73b4fb5549",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Results and Discussion of Results\n",
    "Here is a summary table of the performance of each of our models in terms of cross-validation over the train set (2015-2018 OTPW data) and on the test set (2019 OTPW data).\n",
    "\n",
    "|Algorithm|Train Results|Test Results|\n",
    "|------|------|------|\n",
    "| Majority Class Baseline|- Precision: 55.48% <br> - Recall: 82.05% <br> - Accuracy: 79.38%<br> - F1 Score: 70.35%|- Precision: 64.98% <br> - Recall: 80.61% <br> - Accuracy: 80.61% <br> - F1 Score: 71.96%| \n",
    "|Logistic Classifier|- Precision: 77.68% <br> - Recall: 80.63% <br> - Accuracy: 80.63% <br> - F1 Score: 76.58%|- Precision: 78.62% <br> - Recall: 81.76% <br> - Accuracy: 81.76% <br> - F1 Score: 78.15%| \n",
    "|Random Forest Classifier|- Precision:76.14%  <br> - Recall: 78.36% <br> - Accuracy: 78.34% <br> - F1 Score: 70%|- Precision: 80% <br> - Recall: 80.79% <br> - Accuracy: 80.79% <br> - F1 Score: 72.54%| \n",
    "| Neural Network Classifier | - Precision: 78.27%  <br> - Recall: 88.03 % <br> - Accuracy: 88.03% <br> - F1 Score: 82.65% |- Precision: 76.71% <br> - Recall: 86.31% <br> - Accuracy: 86.31% <br> - F1 Score: 81.23% | \n",
    "\n",
    "The Random Class Baseline model has the lowest test F1 score of 55.14%. Its precision is higher than recall, suggesting it makes fewer false positive predictions but still struggles to identify actual positives effectively. \n",
    "\n",
    "The Majority Class Baseline model has the second lowest test F1 score of 71.96%. The Majority Class Baseline model demonstrates high recall, meaning it identifies most actual positives, but this comes at the cost of lower precision, leading to more false positives. \n",
    "\n",
    "The Logistic Classification model outperforms both baselines with an F1 score of 78.15%. The relatively high precision and recall suggest that this model effectively captures both positive and negative classes, making it a robust option compared to the baseline models. The small difference between training and test results indicates that the model generalizes well, suggesting a balanced fit.\n",
    "\n",
    "The Random Forest Classification model outperforms both baselines with an F1 score of 72.54%. Similar to the Logistic Classification model, the relatively high precision and recall suggest that this model effectively captures both positive and negative classes, making it a robust option compared to the baseline models. The small difference between training and test results indicates that the model generalizes well, suggesting a balanced fit.\n",
    "\n",
    "The Neural Network model outperforms all other models, with the highest F1 score of 81.23%. The small drop from training to testing metrics suggests good generalization and robustness. The high recall (88.03% on train, 86.31% on test) indicates that the model is particularly strong in identifying actual positives, while its precision indicates a lower but acceptable rate of false positives. The overall superior performance across all metrics reflects the neural network’s ability to model complex, non-linear relationships in the data effectively.\n",
    "\n",
    "Interestingly, we observed a slight increase in test set scores compared to training set scores, even after taking measures to prevent data leakage. These findings are intriguing, and in the future, we would like to explore further to understand the underlying reasons for this and what might be contributing to these results.\n",
    "\n",
    "In conclusion, the neural network model's superior metrics suggest that it is the most effective model for predicting flight delays in this dataset, making it the best choice among the models tested.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a659c661-2fe1-4180-a1a2-40fc5f582cf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "To conclude, accurate flight delay predictions can significantly help airlines optimize their resource and personnel allocation to minimize disruption to passengers, make more informed long-term decisions about infrastructure improvements, and contribute to research and development across the broader aviation industry. By leveraging the flight on-time performance data, weather conditions data, and airport metadata from 2015-2019, our objective was to train a binary classification model that effectively predicts whether a flight will experience a significant arrival delay (15 minutes or more).\n",
    "\n",
    "In this project, we constructed Logistic, Random Forest, and Neural Network classification models that had higher F1-scores than our baseline models (always predict the majority class and predict class at random). This indicates that airlines can successfully use these classification models to proactively respond to delays.\n",
    "\n",
    "In the future, it could be valuable to expand our work by incorporating more diverse data (e.g. including international flights to and from the United States or classifying specific types of delays). Another potential avenue for improvement is to develop a model capable of providing real-time predictions with a feedback loop to update the model with new flight data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9a66fc8-3abf-4ce7-8de2-0b3ec505a829",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Code Notebooks\n",
    "* Alec Naidoo: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3133664007185477/command/3133664007185574\n",
    "* Jian Wang: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1183159435935589/command/1183159435935590\n",
    "* Patrick Yim: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3133664007185180/command/3133664007185217\n",
    "* Trisha Sanghal: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4140002248109288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f81c90db-313a-4721-a1fe-2cbb42b14ca5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Team 1-2: Phase 3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
